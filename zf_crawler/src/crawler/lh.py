import time, requests
from datetime import datetime
from bs4 import BeautifulSoup
import json
import os

class LH():

    # ==================================
    # 1. í™˜ê²½ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
    # ==================================

    # ğŸš© í•„í„° ê¸°ì¤€: ê²Œì‹œì¼ >= 2024-11-01
    START_DATE_FILTER = datetime(2024, 11, 1)
    START_DATE_REQUEST = START_DATE_FILTER.strftime("%Y-%m-%d")

    # í˜„ì¬ ë‚ ì§œ (ì¡°íšŒ ì¢…ë£Œì¼)
    TODAY_DATE_REQUEST = datetime.now().strftime("%Y-%m-%d")

    BASE_URL = "https://apply.lh.or.kr/lhapply/apply/wt/wrtanc/selectWrtancList.do"
    DETAIL_URL_BASE = "https://apply.lh.or.kr/lhapply/apply/wt/wrtanc/selectWrtancInfo.do?mi=1026"

    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        "Referer": "https://apply.lh.or.kr/"
    }

    # Payload í…œí”Œë¦¿
    FORM_DATA_TEMPLATE = {
        'currPage': 1,
        'panId': '',
        'cnpCd': '',
        'prevListCo': 50,
        'srchAisTpCd': '',
        'panSs': '',
        'startDt': START_DATE_REQUEST,
        'endDt': TODAY_DATE_REQUEST,
        'aisTpCd': '',
        'uppAisTpCd': '',
        'mi': '',
        'srchUppAisTpCd': '',
        'srchY': 'N',
        'srchFilter': 'N',
        'csCd': '',
        'CNP_CD': '',
        'ccrCnntSysDsCd': '',
        'xssChk': 'N',
        'panEdDt': TODAY_DATE_REQUEST.replace('-', ''),
        'listCo': 1000,
        'panNm': '',
        'indVal': 'N',
        'maxSn': 50,
        'mvinQf': '',
        'netbgn': '',
        'viewType': '',
        'panStDt': '',
        'minSn': 0,
        'schTy': '0',
        'page': 1,
    }

    # ğŸš© ì„ëŒ€/ë¶„ì–‘ ìœ í˜• ëª©ë¡
    LEASE_TYPES = [
        "í†µí•©ê³µê³µì„ëŒ€", "í†µí•©ê³µê³µì„ëŒ€(ì‹ í˜¼í¬ë§)", "êµ­ë¯¼ì„ëŒ€", "ê³µê³µì„ëŒ€", "ì˜êµ¬ì„ëŒ€", "í–‰ë³µì£¼íƒ",
        "í–‰ë³µì£¼íƒ(ì‹ í˜¼í¬ë§)", "ì¥ê¸°ì „ì„¸", "ì‹ ì¶•ë‹¤ì„¸ëŒ€ë§¤ì…ì„ëŒ€", "ë§¤ì…ì„ëŒ€",
        "ì „ì„¸ì„ëŒ€", "ì§‘ì£¼ì¸ì„ëŒ€", "6ë…„ ê³µê³µì„ëŒ€ì£¼íƒ"
    ]

    SALE_TYPES = [
        "ë¶„ì–‘ì£¼íƒ", "ê³µê³µë¶„ì–‘(ì‹ í˜¼í¬ë§)"
    ]

    ETC_TYPE = [
        "ê°€ì •ì–´ë¦°ì´ì§‘"
    ]

    
    def __init__(self):
        pass

    # ==================================
    # 2. í¬ë¡¤ë§ í•µì‹¬ í•¨ìˆ˜
    # ==================================

    def crawl_lh_notices_all_data(self, annc_status:str=None):
        """LH í™ˆí˜ì´ì§€ì—ì„œ 2024-11-01 ì´í›„ì˜ ëª¨ë“  ê³µê³  ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        all_data = []
        page = 1
        list_count = self.FORM_DATA_TEMPLATE['listCo']

        this_title = ""

        print('-'*77)
        print("ğŸš€ LH ê³µê³  ë°ì´í„° ì „ì²´ í¬ë¡¤ë§ ì‹œì‘ (POST ë°©ì‹)...")
        if not annc_status:
            print(f"**í•„í„° ê¸°ì¤€: [ëª¨ë“  ìœ í˜•] + ê²Œì‹œì¼ {self.FORM_DATA_TEMPLATE['startDt']} ì´í›„ ë°ì´í„° ìˆ˜ì§‘.**")
        else:
            print(f"**í•„í„° ê¸°ì¤€: [ìœ í˜• '{annc_status}'] + ê²Œì‹œì¼ {self.FORM_DATA_TEMPLATE['startDt']} ì´í›„ ë°ì´í„° ìˆ˜ì§‘.**")

        while True:
            form_data = self.FORM_DATA_TEMPLATE.copy()
            if annc_status:
                form_data['panSs'] = annc_status
            form_data['currPage'] = str(page)
            form_data['page'] = str(page)
            form_data['minSn'] = str((page - 1) * list_count)
            form_data['maxSn'] = str(page * list_count)
            form_data['prevListCo'] = str(list_count)

            print(f"\nğŸ“„ Crawling page {page} ({form_data['minSn']} to {form_data['maxSn']})...")

            try:
                response = requests.post(self.BASE_URL, data=form_data, headers=self.HEADERS, timeout=15)
                response.raise_for_status()

                # print(response.text)

                # break

                soup = BeautifulSoup(response.text, 'html.parser')
                rows = soup.select("div.bbs_ListA table tbody tr")
                raw_data_count = len(rows)

                if raw_data_count == 0:
                    print("ğŸ ë°ì´í„° ì—†ìŒ. ëª©ë¡ì˜ ëì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
                    break

                new_data_count = 0
                stop_crawling = False

                for row in rows:
                    try:
                        cols = [c.get_text(strip=True) for c in row.select("td")]
                        if len(cols) < 9:
                            continue

                        status = cols[7]
                        post_date_str = cols[5]

                        try:
                            post_date = datetime.strptime(post_date_str, '%Y.%m.%d')
                            if post_date < self.START_DATE_FILTER:
                                print(f"ğŸš© ê²Œì‹œì¼ {post_date_str} ë°ì´í„°ê°€ í•„í„° ê¸°ì¤€({self.START_DATE_REQUEST})ë³´ë‹¤ ì´ì „ì…ë‹ˆë‹¤. ì¶”ê°€ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                                stop_crawling = True
                                break
                        except ValueError:
                            pass

                        title_anchor = row.select_one("td:nth-child(3) a")
                        em_tag = title_anchor.find('em')
                        if em_tag:
                            em_tag.extract()
                        title = title_anchor.get_text(strip=True).replace('\n', ' ')

                        this_title = title

                        """HTML ìš”ì†Œì—ì„œ ìƒì„¸ í˜ì´ì§€ ì¡°íšŒì— í•„ìš”í•œ data ì†ì„±ì„ ì¶”ì¶œí•˜ì—¬ URLì„ ìƒì„±í•©ë‹ˆë‹¤."""
                        data_panId = title_anchor.get('data-id1', '')
                        data_ccr = title_anchor.get('data-id2', '')
                        data_upp = title_anchor.get('data-id3', '')
                        data_ais = title_anchor.get('data-id4', '')

                        if all([data_panId, data_ccr, data_upp, data_ais]):
                            detail_url = (
                                f"{self.DETAIL_URL_BASE}&"
                                f"panId={data_panId}&"
                                f"ccrCnntSysDsCd={data_ccr}&"
                                f"uppAisTpCd={data_upp}&"
                                f"aisTpCd={data_ais}"
                            )
                        else:
                            continue

                        # íŒŒì¼ ì •ë³´ ì—†ëŠ” ê²½ìš° ë¦¬í„´
                        file_down_obj = row.select_one(".listFileDown")

                        if file_down_obj:
                            upp_ais_tp_cd = file_down_obj.get('data-id1', '')
                            ais_tp_cd = file_down_obj.get('data-id2', '')
                            ccr_cnnt_sys_ds_cd = file_down_obj.get('data-id3', '')
                            ls_sst = file_down_obj.get('data-id4', '')
                            pan_id = file_down_obj.get('data-id5', '')
                        else:
                            upp_ais_tp_cd = title_anchor.get('data-id3', '')
                            ais_tp_cd = title_anchor.get('data-id4', '')
                            ccr_cnnt_sys_ds_cd = title_anchor.get('data-id2', '')
                            ls_sst = ""
                            pan_id = title_anchor.get('data-id1', '')


                        # ê³µê³  ìœ í˜• íŒë³„
                        annc_type_simple = 'ê¸°íƒ€'

                        if cols[1] in self.LEASE_TYPES:
                            annc_type_simple = 'ì„ëŒ€'
                        elif cols[1] in self.SALE_TYPES:
                            annc_type_simple = 'ë¶„ì–‘'


                        all_data.append(
                            {
                                'annc_title': this_title, # ê³µê³  ì œëª©
                                'annc_url': detail_url, # ê³µê³  URL
                                'annc_type': annc_type_simple, # ê³µê³  ìœ í˜•
                                'annc_dtl_type': cols[1], # ê³µê³  ìœ í˜• ìƒì„¸
                                'annc_region': cols[3], # ì§€ì—­
                                'annc_pblsh_dt': post_date_str, # ê²Œì‹œì¼
                                'annc_deadline_dt': cols[6], # ë§ˆê°ì¼
                                'annc_status': status, # ê³µê³  ìƒíƒœ
                                'lh_pan_id': pan_id, # ê³µê³  ì‹ë³„ ID
                                'lh_ais_tp_cd': ais_tp_cd, # ê³µê³  ìœ í˜• ì½”ë“œ
                                'lh_upp_ais_tp_cd': upp_ais_tp_cd, # ìƒìœ„ ê³µê³  ìœ í˜• ì½”ë“œ
                                'lh_ccr_cnnt_sys_ds_cd': ccr_cnnt_sys_ds_cd, # ì—°ê³„ ì‹œìŠ¤í…œ êµ¬ë¶„ ì½”ë“œ
                                'lh_ls_sst': ls_sst, # ëª©ë¡ ìƒì˜ ìƒíƒœ/ìˆœì„œ
                            }
                        )


                        new_data_count += 1
                    except Exception as e:
                        print(f"ğŸš¨ ì˜¤ë¥˜: {e} - {this_title}")
                        continue

                print(f"âœ… í˜ì´ì§€ {page} ë¡œë“œ ì„±ê³µ. {len(rows)}ê°œ ì¤‘ {new_data_count}ê°œ ë°ì´í„° ì¶”ì¶œ.")

                if stop_crawling or raw_data_count < list_count:
                    print("ğŸ í¬ë¡¤ë§ ì¢…ë£Œ ì¡°ê±´ ì¶©ì¡±. ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ.")
                    break

                page += 1
                time.sleep(1)

            except requests.exceptions.RequestException as e:
                print(f"ğŸš¨ ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” HTTP ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break
            except Exception as e:
                print(f"ğŸš¨ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break

        if all_data:
            print(f"\nâœ¨ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ. ì´ {len(all_data)}ê±´ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            # return pd.DataFrame(all_data)
            return all_data
        else:
            print("ğŸ‰ ì™„ë£Œ! ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
    

    def get_file_list(self,row):
        
        FILE_CALL_URL = "https://apply.lh.or.kr/lhapply/wt/wrtanc/wrtFileDownl.do"

        # íŒŒì¼ ì¡°íšŒìš© í¬ë¡¤ë§
        form_data_file = {
            'uppAisTpCd1': row['lh_upp_ais_tp_cd'],
            'aisTpCd1': row['lh_ais_tp_cd'],
            'ccrCnntSysDsCd1': row['lh_ccr_cnnt_sys_ds_cd'],
            'lsSst1': row['lh_ls_sst'],
            'panId1': row['lh_pan_id']
        }

        response = requests.post(FILE_CALL_URL, data=form_data_file, headers=self.HEADERS, timeout=15)
        response.encoding = 'utf-8'
        response.raise_for_status()

        file_list = json.loads(response.text)
        
        ok_list = ('ê³µê³ ë¬¸(PDF)',)

        file_list = [obj for obj in file_list if obj['slPanAhflDsCdNm'] in ok_list]

        if len(file_list) == 0:
            raise Exception("íŒŒì¼ ì—†ìŒ!")

        return file_list

    def down_file(self,file_id, file_info={}):

        
        download_url = f'https://apply.lh.or.kr/lhapply/lhFile.do?fileid={file_id}'

        file_response = requests.get(
            download_url,
            stream=True,
            timeout=30,
            verify=False
        )
        file_response.raise_for_status()

        if not file_response.content:
            raise Exception("íŒŒì¼ ë‚´ìš© ì—†ìŒ")

        file_info['file_size'] = file_response.headers.get('Content-Length')
       
        # í´ë”ê°€ ì—†ëŠ” ê²½ìš° ìƒì„±
        os.makedirs("./temp", exist_ok=True)
        
        file_path = "./temp/"+file_info['file_name']

        with open(file_path, mode='wb') as f:
            f.write(file_response.content)

        return file_path, file_info